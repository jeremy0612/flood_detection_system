{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:43:55.921650Z",
     "iopub.status.busy": "2024-10-07T08:43:55.921107Z",
     "iopub.status.idle": "2024-10-07T08:44:04.906827Z",
     "shell.execute_reply": "2024-10-07T08:44:04.905912Z",
     "shell.execute_reply.started": "2024-10-07T08:43:55.921556Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 15:29:58.764261: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-11 15:29:58.788633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-11 15:29:58.815468: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-11 15:29:58.823231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-11 15:29:58.845035: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-11 15:30:00.012724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "import rasterio as rio\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import  cm\n",
    "import cv2\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 10\n",
    "    img_size = (256,256)\n",
    "    BATCH_SIZE = 3\n",
    "    Autotune = tf.data.AUTOTUNE\n",
    "    validation_size = 0.2\n",
    "    class_dict= {0:'No Flooding', \n",
    "                 1: 'Flooding'}\n",
    "    \n",
    "    test_run = False \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "\n",
    "    Read more about the dataset here : https://clmrmb.github.io/SEN12-FLOOD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:44:21.735547Z",
     "iopub.status.busy": "2024-10-07T08:44:21.735074Z",
     "iopub.status.idle": "2024-10-07T08:44:27.735489Z",
     "shell.execute_reply": "2024-10-07T08:44:27.734370Z",
     "shell.execute_reply.started": "2024-10-07T08:44:21.735512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3332, 2237)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_labels = './archive/sen12flood/sen12floods_s1_labels/sen12floods_s1_labels/'\n",
    "s1_tiles = './archive/sen12flood/sen12floods_s1_source/sen12floods_s1_source/'\n",
    "\n",
    "s2_tiles = './archive/sen12flood/sen12floods_s2_source/sen12floods_s2_source/'\n",
    "s2_labels = './archive/sen12flood/sen12floods_s2_labels/sen12floods_s2_labels/'\n",
    "\n",
    "\n",
    "s1_check = 0\n",
    "for file in os.listdir(s1_labels):\n",
    "    if os.path.exists(s1_tiles + '/' + file.replace('labels','source')):\n",
    "        s1_check += 1\n",
    "        \n",
    "         \n",
    "assert s1_check == len(os.listdir(s1_tiles)), 'You my friend , are definintely a idiot!'\n",
    "    \n",
    "s2_check = 0\n",
    "for file in os.listdir(s2_labels):\n",
    "    if os.path.exists(s2_tiles + '/' + file.replace('labels','source')):\n",
    "        s2_check += 1\n",
    "        \n",
    "        \n",
    "assert s2_check == len(os.listdir(s2_tiles)), 'You my friend , are definintely  the idiot!'\n",
    "\n",
    "\n",
    "s1_check,s2_check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a dataset of paths and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:45:22.673590Z",
     "iopub.status.busy": "2024-10-07T08:45:22.673126Z",
     "iopub.status.idle": "2024-10-07T08:45:22.680997Z",
     "shell.execute_reply": "2024-10-07T08:45:22.679761Z",
     "shell.execute_reply.started": "2024-10-07T08:45:22.673555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    '''loads a json file'''\n",
    "    with open(path,'r') as file:\n",
    "        js = json.load(file)\n",
    "        \n",
    "    return js\n",
    "\n",
    "# collectionss1 = load_json('../input/sen12flood/sen12flood/sen12floods_s1_source/sen12floods_s1_source/collection.json')\n",
    "# collections2= load_json('../input/sen12flood/sen12flood/sen12floods_s2_source/sen12floods_s2_source/collection.json')\n",
    "# collections2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:45:26.008975Z",
     "iopub.status.busy": "2024-10-07T08:45:26.008528Z",
     "iopub.status.idle": "2024-10-07T08:45:26.024787Z",
     "shell.execute_reply": "2024-10-07T08:45:26.023677Z",
     "shell.execute_reply.started": "2024-10-07T08:45:26.008938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_label_json(label_json):\n",
    "    '''process a single label json'''\n",
    "    info_dict = {}\n",
    "    \n",
    "    info_dict['geometry'] = label_json['geometry']['coordinates']\n",
    "    info_dict['label'] = label_json['properties']['FLOODING']\n",
    "    info_dict['date'] = label_json['properties']['date']\n",
    "    info_dict['tile_number'] = label_json['properties']['tile']\n",
    "#     info_dict['full_data_coverage']= label_json['properties']['FULL-DATA-COVERAGE']\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "\n",
    "def process_label_stac(stac_json):\n",
    "    return stac_json['id']\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def image_path_from_label_dir(image_parent_dir:str,\n",
    "                              label_file :str)->str:\n",
    "    \n",
    "    return image_parent_dir + '/' + label_file.replace('labels','source')\n",
    "    \n",
    "    \n",
    "\n",
    "def process_json(label_path,image_directory):\n",
    "    '''get the data for a single example\n",
    "     Inputs \n",
    "     label_path : path to the label folder \n",
    "     image_directory: path to the corresponding image directory'''\n",
    "    \n",
    "    \n",
    "\n",
    "    #get image directory for that label\n",
    "    folder_id = label_path.rsplit('/',1)[1]\n",
    "    image_dir_path = image_path_from_label_dir(image_directory,folder_id)\n",
    "\n",
    "    if not os.path.exists(image_dir_path):\n",
    "        return {'File_not_found':image_dir_path}\n",
    "    \n",
    "    \n",
    "    for file in os.listdir(label_path):\n",
    "        #if image dir exists \n",
    "        if file.startswith('labels'):\n",
    "            label_json = load_json(os.path.join(label_path,file))\n",
    "        else:\n",
    "            stac_json = load_json(os.path.join(label_path,file))\n",
    "\n",
    "\n",
    "    #get data \n",
    "    info_dict = process_label_json(label_json)\n",
    "\n",
    "    #get id \n",
    "    info_dict['id'] = process_label_stac(stac_json)\n",
    "    \n",
    "    #location id \n",
    "    info_dict['location_id'] = info_dict['id'].split('_')[3]\n",
    "    \n",
    "    \n",
    "    info_dict['image_dir'] = image_dir_path\n",
    "    \n",
    "    \n",
    "    return info_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:45:26.847646Z",
     "iopub.status.busy": "2024-10-07T08:45:26.847172Z",
     "iopub.status.idle": "2024-10-07T08:45:26.858432Z",
     "shell.execute_reply": "2024-10-07T08:45:26.857133Z",
     "shell.execute_reply.started": "2024-10-07T08:45:26.847609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataframe(label_directory,image_directory):\n",
    "    '''get dataframe from the nested label directory'''\n",
    "    records = []\n",
    "    \n",
    "        \n",
    "    for folder in os.listdir(label_directory):\n",
    "        if folder.startswith('sen12'):\n",
    "#             print(folder,label_directory)\n",
    "            folder_path = label_directory + '/' + folder\n",
    "            \n",
    "            \n",
    "            #get data for a single example\n",
    "            feature = process_json(label_path=folder_path,\n",
    "                                   image_directory=image_directory)\n",
    "            \n",
    "            \n",
    "            records.append(feature)\n",
    "            \n",
    "            \n",
    "    return pd.DataFrame.from_records(data = records)\n",
    "\n",
    "\n",
    "\n",
    "def type_cast_dataset(dataset):\n",
    "    '''typecasting columns in dataset'''\n",
    "    dataset['label'] = dataset['label'].astype(int)\n",
    "    \n",
    "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "    dataset['tile_number'] = dataset['tile_number'].astype('int8')\n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:45:28.058992Z",
     "iopub.status.busy": "2024-10-07T08:45:28.058512Z",
     "iopub.status.idle": "2024-10-07T08:46:22.320395Z",
     "shell.execute_reply": "2024-10-07T08:46:22.319239Z",
     "shell.execute_reply.started": "2024-10-07T08:45:28.058951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique locations in Sentinel1 (SAR) data : 335\n",
      "Number of unique locations in Sentinel2 (optical) data : 335\n",
      "CPU times: user 357 ms, sys: 193 ms, total: 550 ms\n",
      "Wall time: 549 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3331, 7), (2236, 7))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "s1_data = type_cast_dataset(\n",
    "                            get_dataframe(\n",
    "                                label_directory=s1_labels,\n",
    "                                image_directory=s1_tiles\n",
    "                                        )\n",
    "                            )\n",
    "\n",
    "\n",
    "s2_data = type_cast_dataset(\n",
    "                            get_dataframe(label_directory=s2_labels,\n",
    "                                          image_directory=s2_tiles)\n",
    "                            )\n",
    "\n",
    "print(f'Number of unique locations in Sentinel1 (SAR) data : {s1_data.location_id.nunique()}')\n",
    "print(f'Number of unique locations in Sentinel2 (optical) data : {s2_data.location_id.nunique()}')\n",
    "\n",
    "s1_data.shape,s2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:46:22.322957Z",
     "iopub.status.busy": "2024-10-07T08:46:22.322475Z",
     "iopub.status.idle": "2024-10-07T08:46:22.540587Z",
     "shell.execute_reply": "2024-10-07T08:46:22.539232Z",
     "shell.execute_reply.started": "2024-10-07T08:46:22.322908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# saving datasets\n",
    "s1_data.to_csv('s1_data.csv',index=False)\n",
    "s2_data.to_csv('s2_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:48:26.856898Z",
     "iopub.status.busy": "2024-10-07T08:48:26.856411Z",
     "iopub.status.idle": "2024-10-07T08:48:26.862871Z",
     "shell.execute_reply": "2024-10-07T08:48:26.861905Z",
     "shell.execute_reply.started": "2024-10-07T08:48:26.856856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_raster(filepath):\n",
    "    '''load a single band raster'''\n",
    "    with rio.open(filepath) as file: \n",
    "        raster = file.read().squeeze(axis=0)\n",
    "        \n",
    "    return raster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading multiple raster bands as single raster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:48:29.082011Z",
     "iopub.status.busy": "2024-10-07T08:48:29.081585Z",
     "iopub.status.idle": "2024-10-07T08:48:29.103853Z",
     "shell.execute_reply": "2024-10-07T08:48:29.102670Z",
     "shell.execute_reply.started": "2024-10-07T08:48:29.081977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_s1_tiffs(folder,\n",
    "                  scaling_values=[50.,100.]):\n",
    "    images = []\n",
    "    i = 0\n",
    "    for im in sorted(os.listdir(folder)):\n",
    "         \n",
    "        if im.rsplit('.',maxsplit=1)[1] == 'tif':\n",
    "            \n",
    "            path = folder + '/' + im\n",
    "            band = load_raster(path)\n",
    "            band = band / scaling_values[i]\n",
    "            \n",
    "            band = cv2.resize(band,\n",
    "                              CFG.img_size)\n",
    "            \n",
    "            images.append(band)\n",
    "            i+=1 \n",
    "                    \n",
    "    return np.dstack(images)\n",
    "\n",
    "\n",
    "def load_s2_tiffs(folder,\n",
    "                  scaling_value=10000.):\n",
    "    images = []\n",
    "    for im in sorted(os.listdir(folder)):\n",
    "        if im.rsplit('.',maxsplit=1)[1] == 'tif':    \n",
    "            path = folder + '/' + im\n",
    "            band = load_raster(path)\n",
    "            band = band/ scaling_value\n",
    "            \n",
    "            band = cv2.resize(band,CFG.img_size)\n",
    "            images.append(band)   \n",
    "\n",
    "    return np.dstack(images)\n",
    "                    \n",
    "def load_rgb_tiffs(folder,\n",
    "                  scaling_value=10000.):\n",
    "    '''load R,G and B bands'''\n",
    "    \n",
    "    images = []\n",
    "    for im in sorted(os.listdir(folder)):\n",
    "        name,file_format = im.rsplit('.',maxsplit=1)\n",
    "        if ((file_format== 'tif') and (name in ['B02','B03','B04'])):    \n",
    "            path = folder + '/' + im\n",
    "            band = load_raster(path)\n",
    "            band = band/ scaling_value\n",
    "            \n",
    "            band = cv2.resize(band,CFG.img_size)\n",
    "            images.append(band)   \n",
    "\n",
    "    return np.dstack(images)[:,:,::-1]\n",
    "\n",
    "\n",
    "    \n",
    "def tf_load_s1(path):    \n",
    "    path = path.numpy().decode('utf-8')\n",
    "    return load_s1_tiffs(path)\n",
    "    \n",
    "    \n",
    "\n",
    "def tf_load_s2(path):    \n",
    "    path = path.numpy().decode('utf-8')\n",
    "    return load_s2_tiffs(path)\n",
    "\n",
    "\n",
    "def tf_load_rgb(path):    \n",
    "    path = path.numpy().decode('utf-8')\n",
    "    return load_rgb_tiffs(path)\n",
    "    \n",
    "def process_image_s1(filename):\n",
    "    '''function for preprocessing in tensorflow data'''\n",
    "    \n",
    "    image = tf.py_function(tf_load_s1, [filename], tf.float32)\n",
    "    image.set_shape([None, None, 2])  # Explicitly set the shape (assuming grayscale images)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def process_image_s2(filename):\n",
    "    '''function for preprocessing in tensorflow data'''\n",
    "    \n",
    "    return tf.py_function(tf_load_s2, \n",
    "                          [filename], \n",
    "                          tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "def process_image_rgb(filename):\n",
    "    '''function for preprocessing in tensorflow data'''\n",
    "    \n",
    "    image = tf.py_function(tf_load_rgb, \n",
    "                          [filename], \n",
    "                          tf.float32)\n",
    "    image.set_shape([None,None,3])\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:48:30.444343Z",
     "iopub.status.busy": "2024-10-07T08:48:30.443771Z",
     "iopub.status.idle": "2024-10-07T08:48:40.976199Z",
     "shell.execute_reply": "2024-10-07T08:48:40.975083Z",
     "shell.execute_reply.started": "2024-10-07T08:48:30.444294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_rasters_in_folder(path):\n",
    "    count = 0 \n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        if file.rsplit('.',1)[1] == 'tif':\n",
    "            count +=1 \n",
    "            \n",
    "    return count \n",
    "    \n",
    "    \n",
    "s2_data['raster_count'] = s2_data.image_dir.apply(lambda x : count_rasters_in_folder(x))\n",
    "\n",
    "#value counts \n",
    "s2_data['raster_count'].value_counts()\n",
    "\n",
    "\n",
    "s2_data=s2_data[s2_data['raster_count']==12] # take only valid rasters\n",
    "# s2_data[s2_data['raster_count']==0]['location_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a TF dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    First lets split the dataset into training and validation set. We will stratify based on location id to ensure that locations are well represented in traininng and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:21.520212Z",
     "iopub.status.busy": "2024-10-07T08:52:21.519127Z",
     "iopub.status.idle": "2024-10-07T08:52:21.538309Z",
     "shell.execute_reply": "2024-10-07T08:52:21.537215Z",
     "shell.execute_reply.started": "2024-10-07T08:52:21.520167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2126, 8), (12, 8))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isolating single loaction ids (as they will be a problem for stratification)\n",
    "\n",
    "# single example locations \n",
    "single_index = s2_data['location_id'].value_counts()[s2_data['location_id'].value_counts()==1].index\n",
    "\n",
    "single_index_df = s2_data[s2_data['location_id'].isin(single_index)].reset_index(drop=True)\n",
    "s2_data0 = s2_data[~(s2_data['location_id'].isin(single_index))].reset_index(drop=True)\n",
    "\n",
    "s2_data0.shape,single_index_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split dataset into train and validation splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:22.565479Z",
     "iopub.status.busy": "2024-10-07T08:52:22.565012Z",
     "iopub.status.idle": "2024-10-07T08:52:23.024352Z",
     "shell.execute_reply": "2024-10-07T08:52:23.023312Z",
     "shell.execute_reply.started": "2024-10-07T08:52:22.565442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split  # Assuming you have scikit-learn installed\n",
    "\n",
    "# Train-test split with stratification\n",
    "s1_data_tr, s1_data_val = train_test_split(s1_data,\n",
    "                                          test_size=CFG.validation_size,\n",
    "                                          random_state=CFG.seed,\n",
    "                                          stratify=s1_data.location_id)\n",
    "\n",
    "s2_data_tr, s2_data_val = train_test_split(s2_data0,\n",
    "                                          test_size=CFG.validation_size,\n",
    "                                          random_state=CFG.seed,\n",
    "                                          stratify=s2_data0.location_id)\n",
    "\n",
    "# Concatenate s2_data_tr and single_index_df while resetting index\n",
    "s2_data_tr = pd.concat([s2_data_tr, single_index_df], ignore_index=True)\n",
    "\n",
    "# No need for del or gc.collect() here (automatic memory management)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:23.026279Z",
     "iopub.status.busy": "2024-10-07T08:52:23.025887Z",
     "iopub.status.idle": "2024-10-07T08:52:23.037684Z",
     "shell.execute_reply": "2024-10-07T08:52:23.036671Z",
     "shell.execute_reply.started": "2024-10-07T08:52:23.026228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label\n",
       " 0    0.693318\n",
       " 1    0.306682\n",
       " Name: proportion, dtype: float64,\n",
       " label\n",
       " 0    0.67916\n",
       " 1    0.32084\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_data_tr.label.value_counts(1),s1_data_val.label.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:23.243471Z",
     "iopub.status.busy": "2024-10-07T08:52:23.242639Z",
     "iopub.status.idle": "2024-10-07T08:52:23.254503Z",
     "shell.execute_reply": "2024-10-07T08:52:23.253323Z",
     "shell.execute_reply.started": "2024-10-07T08:52:23.243427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label\n",
       " 0    0.768107\n",
       " 1    0.231893\n",
       " Name: proportion, dtype: float64,\n",
       " label\n",
       " 0    0.746479\n",
       " 1    0.253521\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2_data_tr.label.value_counts(1),s2_data_val.label.value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for image augmentations**\n",
    "\n",
    "    Although the Augmentations are simple, we cannot use them on SAR images , as even simple operations like flipping can change the meaning of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:24.096585Z",
     "iopub.status.busy": "2024-10-07T08:52:24.095762Z",
     "iopub.status.idle": "2024-10-07T08:52:24.107807Z",
     "shell.execute_reply": "2024-10-07T08:52:24.106537Z",
     "shell.execute_reply.started": "2024-10-07T08:52:24.096544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def augment_image_multispectral(image):\n",
    "    '''perform simple image augmentations'''\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_crop(image, size=(*CFG.img_size,12))\n",
    "    \n",
    "    rot = tf.random.normal((1,),mean = 0.35, stddev=0.15)\n",
    "    \n",
    "    if rot > 0.5:\n",
    "        image = tf.image.rot90(image)\n",
    "\n",
    "    return image \n",
    "\n",
    "def augment_image(image):\n",
    "    '''perform simple image augmentations'''\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_crop(image, size=(*CFG.img_size,3))\n",
    "    \n",
    "    rot = tf.random.normal((1,),mean = 0.35, stddev=0.15)\n",
    "    \n",
    "    if rot > 0.5:\n",
    "        image = tf.image.rot90(image)\n",
    "\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:24.573492Z",
     "iopub.status.busy": "2024-10-07T08:52:24.573031Z",
     "iopub.status.idle": "2024-10-07T08:52:24.586216Z",
     "shell.execute_reply": "2024-10-07T08:52:24.585266Z",
     "shell.execute_reply.started": "2024-10-07T08:52:24.573454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_tf_dataset(image_paths,\n",
    "                   labels=None, # put none for test data set\n",
    "                   image_processing_fn=None,\n",
    "                   augment_fn = None\n",
    "                  ):\n",
    "    \n",
    "    '''returns a tf dataset object\n",
    "    Inputs: \n",
    "    image_paths : paths to images\n",
    "    labels: labels of each image\n",
    "    image_processing_fn:  function to load and preprocess images \n",
    "    augment_fn : function to augment images '''\n",
    "    \n",
    "    #seperate datasets\n",
    "    if labels is not None:\n",
    "        labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    #load images \n",
    "    image_dataset = image_dataset.map(image_processing_fn,\n",
    "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     \n",
    "    if augment_fn is not None:\n",
    "        \n",
    "        image_dataset = image_dataset.map(augment_fn,\n",
    "                                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     \n",
    "    \n",
    "    if labels is not None:\n",
    "        return tf.data.Dataset.zip((image_dataset,labels_dataset))\n",
    "    \n",
    "    \n",
    "    return image_dataset\n",
    "\n",
    "\n",
    "\n",
    "def optimize_pipeline(tf_dataset,\n",
    "                      batch_size = CFG.BATCH_SIZE,\n",
    "                      Autotune_fn = CFG.Autotune,\n",
    "                      cache= False,\n",
    "                      batch = True):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # prefetch(load the data with cpu,while gpu is training) the data in memory \n",
    "    tf_dataset = tf_dataset.prefetch(buffer_size=Autotune_fn)  \n",
    "    if cache:\n",
    "        tf_dataset = tf_dataset.cache()                        # store data in RAM  \n",
    "        \n",
    "    tf_dataset =  tf_dataset.shuffle(buffer_size=50)         # shuffle \n",
    "    \n",
    "    if batch:\n",
    "        tf_dataset = tf_dataset.batch(batch_size)              #split the data in batches  \n",
    "    \n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making dataset pipelines with TF data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:26.179360Z",
     "iopub.status.busy": "2024-10-07T08:52:26.178908Z",
     "iopub.status.idle": "2024-10-07T08:52:26.327532Z",
     "shell.execute_reply": "2024-10-07T08:52:26.326491Z",
     "shell.execute_reply.started": "2024-10-07T08:52:26.179322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 15:30:03.403462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 36383 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:3b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Sentinel 1 dataset (not using augmentation here)\n",
    "\n",
    "S1_dataset_tr = optimize_pipeline(tf_dataset=get_tf_dataset(image_paths = s1_data_tr.image_dir.values,\n",
    "                                               labels = tf.one_hot(s1_data_tr.label,depth=2),\n",
    "                                               image_processing_fn = process_image_s1),\n",
    "                                  \n",
    "                                  batch_size = 3 * CFG.BATCH_SIZE)\n",
    "\n",
    "\n",
    "S1_dataset_val = optimize_pipeline(tf_dataset=get_tf_dataset(image_paths = s1_data_val.image_dir.values,\n",
    "                                                labels = tf.one_hot(s1_data_val.label,depth=2),\n",
    "                                                image_processing_fn = process_image_s1),\n",
    "                                   batch_size = 3* CFG.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:26.818416Z",
     "iopub.status.busy": "2024-10-07T08:52:26.817495Z",
     "iopub.status.idle": "2024-10-07T08:52:27.363233Z",
     "shell.execute_reply": "2024-10-07T08:52:27.362302Z",
     "shell.execute_reply.started": "2024-10-07T08:52:26.818369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#sentinel 2 dataset \n",
    "S2_dataset_tr = optimize_pipeline(get_tf_dataset(image_paths = s2_data_tr.image_dir.values,\n",
    "                                                   labels = s2_data_tr.label,\n",
    "                                                   image_processing_fn = process_image_s2,\n",
    "                                                   augment_fn = augment_image_multispectral)\n",
    "                                 )\n",
    "\n",
    "\n",
    "S2_dataset_val = optimize_pipeline(get_tf_dataset(image_paths = s2_data_val.image_dir.values,\n",
    "                                                   labels = s2_data_val.label,\n",
    "                                                   image_processing_fn = process_image_s2,\n",
    "                                                   augment_fn = augment_image_multispectral)\n",
    "                                  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:27.365484Z",
     "iopub.status.busy": "2024-10-07T08:52:27.365092Z",
     "iopub.status.idle": "2024-10-07T08:52:27.869739Z",
     "shell.execute_reply": "2024-10-07T08:52:27.868325Z",
     "shell.execute_reply.started": "2024-10-07T08:52:27.365448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RGB_dataset_tr = optimize_pipeline(get_tf_dataset(image_paths = s2_data_tr.image_dir.values,\n",
    "                                                   labels = s2_data_tr.label,\n",
    "                                                   image_processing_fn = process_image_rgb,\n",
    "                                                   augment_fn = augment_image),\n",
    "                                   batch_size = 3* CFG.BATCH_SIZE\n",
    "                                 )\n",
    "\n",
    "\n",
    "RGB_dataset_val = optimize_pipeline(get_tf_dataset(image_paths = s2_data_val.image_dir.values,\n",
    "                                                   labels = s2_data_val.label,\n",
    "                                                   image_processing_fn = process_image_rgb,\n",
    "                                                   augment_fn = augment_image),\n",
    "                                    batch_size = 3* CFG.BATCH_SIZE\n",
    "                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking to see if the Pipelines work as expected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of SAR dataset input(val) (9, 256, 256, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 15:30:06.713587: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for x,y in S1_dataset_val.take(1): # take one batch for checking \n",
    "    print(f'shape of SAR dataset input(val) {x.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:28.523314Z",
     "iopub.status.busy": "2024-10-07T08:52:28.522767Z",
     "iopub.status.idle": "2024-10-07T08:52:29.790239Z",
     "shell.execute_reply": "2024-10-07T08:52:29.789060Z",
     "shell.execute_reply.started": "2024-10-07T08:52:28.523273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of SAR dataset input (9, 256, 256, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 15:30:07.837010: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for x,y in S1_dataset_tr.take(1): # take one batch for checking \n",
    "    print(f'shape of SAR dataset input {x.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:29.792680Z",
     "iopub.status.busy": "2024-10-07T08:52:29.792321Z",
     "iopub.status.idle": "2024-10-07T08:52:33.980508Z",
     "shell.execute_reply": "2024-10-07T08:52:33.979220Z",
     "shell.execute_reply.started": "2024-10-07T08:52:29.792647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of MultiSpectral dataset input (3, 256, 256, 12)\n"
     ]
    }
   ],
   "source": [
    "for x,y in S2_dataset_tr.take(1): # take one batch for checking \n",
    "    print(f'shape of MultiSpectral dataset input {x.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:33.983477Z",
     "iopub.status.busy": "2024-10-07T08:52:33.983091Z",
     "iopub.status.idle": "2024-10-07T08:52:35.546659Z",
     "shell.execute_reply": "2024-10-07T08:52:35.545605Z",
     "shell.execute_reply.started": "2024-10-07T08:52:33.983444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of MultiSpectral dataset input (9, 256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 15:30:19.236041: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for x,y in RGB_dataset_tr.take(1): # take one batch for checking \n",
    "    print(f'shape of MultiSpectral dataset input {x.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CNN Models    CNN models to identify flooding in opotical and SAR images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multichannel_cnn(num_channels:int,\n",
    "                     hidden_units:int, #number of  hidden dense \n",
    "                     weights = None  # none for random init, use imagenet for imagenet weights \n",
    "                    ):\n",
    "    '''model that takes multiple channel as input, instead of using the rgb channels as by default'''\n",
    "    \n",
    "    \n",
    "    # backbone = tf.keras.applications.resnet_v2.ResNet50V2(\n",
    "    #                                         include_top=False,\n",
    "    #                                         input_shape = (*CFG.img_size,num_channels),\n",
    "    #                                         weights=weights,\n",
    "    #                                         pooling = 'avg')\n",
    "    \n",
    "    def add_constant_channel(x):\n",
    "        # Create a constant tensor with the same batch size and spatial dimensions, and value 0\n",
    "        constant_channel = tf.zeros_like(x[:, :, :, :1])  # Shape (224, 224, 1)\n",
    "        # Concatenate the constant channel to the existing two channels\n",
    "        return tf.concat([x, constant_channel], axis=-1)\n",
    "\n",
    "    if num_channels == 2:\n",
    "    # Apply the lambda layer\n",
    "        input_layer = tf.keras.layers.Input(shape=(*CFG.img_size, 2))\n",
    "        input_tensor = tf.keras.layers.Lambda(add_constant_channel)(input_layer)\n",
    "    else:\n",
    "        input_tensor = tf.keras.layers.Input(shape=(*CFG.img_size, num_channels))\n",
    "        \n",
    "    backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "                                            include_top=False,\n",
    "                                            input_tensor=input_tensor,\n",
    "                                            input_shape = input_tensor.shape[0],\n",
    "                                            weights='imagenet',\n",
    "                                            pooling = 'avg')\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(backbone.output)\n",
    "    x = tf.keras.layers.Dense(hidden_units, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(rate = 0.2)(x)\n",
    "\n",
    "    final_out = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
    "    \n",
    "    #make a model \n",
    "    model = tf.keras.Model(inputs = backbone.input, \n",
    "                  outputs = final_out)\n",
    "    \n",
    "    return model \n",
    "\n",
    "# plot train and val acc as  a function of epochs\n",
    "def plot_history(history,addn_metric=None):\n",
    "    '''\n",
    "    Inputs\n",
    "    history:history object from tensorflow\n",
    "    add_metric: metric name in the history (like f1_score)'''\n",
    "    his=pd.DataFrame(history.history)\n",
    "    \n",
    "    if addn_metric:\n",
    "        plt.subplots(1,3,figsize=(20,6))\n",
    "        #loss:\n",
    "        ax1=plt.subplot(1,3,1)\n",
    "        ax1.plot(range(len(his)),his['loss'],color='g',label='training')\n",
    "        ax1.plot(range(len(his)),his['val_loss'],color='r',label='validation')\n",
    "        ax1.set_xlabel('EPOCHS')\n",
    "        ax1.set_ylabel('LOSS')\n",
    "        ax1.legend()\n",
    "        ax1.set_title('Loss Per Epoch')\n",
    "        #accuracy\n",
    "        ax2=plt.subplot(1,3,2)\n",
    "        ax2.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')\n",
    "        ax2.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')\n",
    "        ax2.set_xlabel('EPOCHS')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.set_title('Accuracy Per Epoch')\n",
    "        \n",
    "        ax3= plt.subplot(1,3,3)\n",
    "        ax3.plot(range(len(his)),his[f'{addn_metric}'],color='g',label='training')\n",
    "        ax3.plot(range(len(his)),his[f'val_{addn_metric}'],color='r',label='validation')\n",
    "        ax3.set_xlabel('EPOCHS')\n",
    "        ax3.set_ylabel(f'{addn_metric}')\n",
    "        ax3.legend()\n",
    "        ax3.set_title(f'{addn_metric} Per Epoch')\n",
    "    else:\n",
    "        plt.subplots(1,2,figsize=(20,8))\n",
    "        #loss:\n",
    "        ax1=plt.subplot(1,2,1)\n",
    "        ax1.plot(range(len(his)),his['loss'],color='g',label='training')\n",
    "        ax1.plot(range(len(his)),his['val_loss'],color='r',label='validation')\n",
    "        ax1.set_xlabel('EPOCHS')\n",
    "        ax1.set_ylabel('LOSS')\n",
    "        ax1.legend()\n",
    "        ax1.set_title('Loss Per Epoch')\n",
    "        #accuracy\n",
    "        ax2=plt.subplot(1,2,2)\n",
    "        ax2.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')\n",
    "        ax2.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')\n",
    "        ax2.set_xlabel('EPOCHS')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.set_title('Accuracy Per Epoch')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:35.581538Z",
     "iopub.status.busy": "2024-10-07T08:52:35.581142Z",
     "iopub.status.idle": "2024-10-07T08:52:35.599372Z",
     "shell.execute_reply": "2024-10-07T08:52:35.598316Z",
     "shell.execute_reply.started": "2024-10-07T08:52:35.581503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#from https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, dtype='float32')  # Ensure both y_true and y_pred are float32\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, dtype='float32')  # Ensure both y_true and y_pred are float32\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:35.600982Z",
     "iopub.status.busy": "2024-10-07T08:52:35.600584Z",
     "iopub.status.idle": "2024-10-07T08:52:38.152181Z",
     "shell.execute_reply": "2024-10-07T08:52:38.150713Z",
     "shell.execute_reply.started": "2024-10-07T08:52:35.600948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3174113/617266309.py:27: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
      "/home/ea301b/anaconda3/envs/tensor/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_163']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SAR_CNN = multichannel_cnn(num_channels = 2,\n",
    "                           hidden_units = 512, #number of  hidden dense \n",
    "                          )\n",
    "\n",
    "\n",
    "\n",
    "SAR_CNN.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                metrics = ['accuracy',f1_score,recall_m,precision_m]\n",
    "               )\n",
    "\n",
    "#check on some data \n",
    "for x,y in S1_dataset_val.take(1): # take one batch for checking \n",
    "    SAR_CNN(x)\n",
    "\n",
    "\n",
    "# !mkdir CNN_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(SAR_CNN, to_file=\"multichannel_cnn_architecture.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:38.155900Z",
     "iopub.status.busy": "2024-10-07T08:52:38.154743Z",
     "iopub.status.idle": "2024-10-07T08:52:38.166802Z",
     "shell.execute_reply": "2024-10-07T08:52:38.165605Z",
     "shell.execute_reply.started": "2024-10-07T08:52:38.155835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 2 if CFG.test_run else 75\n",
    "# callbacks \n",
    "#reduce_learning rate\n",
    "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(patience=5,\n",
    "                                                factor=0.8,\n",
    "                                                min_delta=1e-2,\n",
    "                                                monitor='val_accuracy',\n",
    "                                                verbose=1,\n",
    "                                                mode='max')\n",
    "\n",
    "#early stopping \n",
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                              min_delta=1e-3,\n",
    "                                              monitor='val_accuracy',\n",
    "                                              restore_best_weights=True,\n",
    "                                              mode='max')\n",
    "\n",
    "\n",
    "# exponential decay \n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    '''learning rate scheduler, decays expo after the tenth epoch'''\n",
    "\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))\n",
    "    \n",
    "\n",
    "    \n",
    "learning_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "\n",
    "callbacks_1= [reduce_lr,early_stopping,learning_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training the SAR CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training on SAR data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T08:52:38.168879Z",
     "iopub.status.busy": "2024-10-07T08:52:38.168449Z",
     "iopub.status.idle": "2024-10-07T15:27:41.965867Z",
     "shell.execute_reply": "2024-10-07T15:27:41.961613Z",
     "shell.execute_reply.started": "2024-10-07T08:52:38.168843Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 153ms/step - accuracy: 0.6652 - f1_score: 0.6652 - loss: 0.8314 - precision_m: 0.6652 - recall_m: 0.6652 - val_accuracy: 0.4588 - val_f1_score: 0.4652 - val_loss: 1.9344 - val_precision_m: 0.4652 - val_recall_m: 0.4652 - learning_rate: 1.0000e-04\n",
      "Epoch 2/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - accuracy: 0.7512 - f1_score: 0.7512 - loss: 0.5922 - precision_m: 0.7512 - recall_m: 0.7512 - val_accuracy: 0.4903 - val_f1_score: 0.4963 - val_loss: 0.6578 - val_precision_m: 0.4963 - val_recall_m: 0.4963 - learning_rate: 1.0000e-04\n",
      "Epoch 3/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - accuracy: 0.7857 - f1_score: 0.7857 - loss: 0.5035 - precision_m: 0.7857 - recall_m: 0.7857 - val_accuracy: 0.3973 - val_f1_score: 0.4044 - val_loss: 0.8285 - val_precision_m: 0.4044 - val_recall_m: 0.4044 - learning_rate: 1.0000e-04\n",
      "Epoch 4/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - accuracy: 0.7984 - f1_score: 0.7984 - loss: 0.4640 - precision_m: 0.7984 - recall_m: 0.7984 - val_accuracy: 0.6702 - val_f1_score: 0.6622 - val_loss: 0.5984 - val_precision_m: 0.6622 - val_recall_m: 0.6622 - learning_rate: 1.0000e-04\n",
      "Epoch 5/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - accuracy: 0.8309 - f1_score: 0.8309 - loss: 0.3972 - precision_m: 0.8309 - recall_m: 0.8309 - val_accuracy: 0.6792 - val_f1_score: 0.6711 - val_loss: 0.7566 - val_precision_m: 0.6711 - val_recall_m: 0.6711 - learning_rate: 1.0000e-04\n",
      "Epoch 6/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 131ms/step - accuracy: 0.8406 - f1_score: 0.8406 - loss: 0.3836 - precision_m: 0.8406 - recall_m: 0.8406 - val_accuracy: 0.6792 - val_f1_score: 0.6830 - val_loss: 0.7385 - val_precision_m: 0.6830 - val_recall_m: 0.6830 - learning_rate: 1.0000e-04\n",
      "Epoch 7/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - accuracy: 0.8436 - f1_score: 0.8436 - loss: 0.3726 - precision_m: 0.8436 - recall_m: 0.8436 - val_accuracy: 0.6792 - val_f1_score: 0.6830 - val_loss: 0.7693 - val_precision_m: 0.6830 - val_recall_m: 0.6830 - learning_rate: 1.0000e-04\n",
      "Epoch 8/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 132ms/step - accuracy: 0.8531 - f1_score: 0.8531 - loss: 0.3638 - precision_m: 0.8531 - recall_m: 0.8531 - val_accuracy: 0.6792 - val_f1_score: 0.6711 - val_loss: 0.7348 - val_precision_m: 0.6711 - val_recall_m: 0.6711 - learning_rate: 1.0000e-04\n",
      "Epoch 9/75\n",
      "\u001b[1m295/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8519 - f1_score: 0.8519 - loss: 0.3203 - precision_m: 0.8519 - recall_m: 0.8519\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 129ms/step - accuracy: 0.8519 - f1_score: 0.8519 - loss: 0.3203 - precision_m: 0.8519 - recall_m: 0.8519 - val_accuracy: 0.6792 - val_f1_score: 0.6830 - val_loss: 1.2590 - val_precision_m: 0.6830 - val_recall_m: 0.6830 - learning_rate: 8.0000e-05\n",
      "Epoch 10/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 128ms/step - accuracy: 0.8836 - f1_score: 0.8836 - loss: 0.2808 - precision_m: 0.8836 - recall_m: 0.8836 - val_accuracy: 0.6792 - val_f1_score: 0.6711 - val_loss: 1.6129 - val_precision_m: 0.6711 - val_recall_m: 0.6711 - learning_rate: 8.0000e-05\n",
      "Epoch 11/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 141ms/step - accuracy: 0.8895 - f1_score: 0.8895 - loss: 0.2585 - precision_m: 0.8895 - recall_m: 0.8895 - val_accuracy: 0.6882 - val_f1_score: 0.6800 - val_loss: 1.1504 - val_precision_m: 0.6800 - val_recall_m: 0.6800 - learning_rate: 7.2387e-05\n",
      "Epoch 12/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 143ms/step - accuracy: 0.8990 - f1_score: 0.8990 - loss: 0.2589 - precision_m: 0.8990 - recall_m: 0.8990 - val_accuracy: 0.6987 - val_f1_score: 0.7022 - val_loss: 1.0027 - val_precision_m: 0.7022 - val_recall_m: 0.7022 - learning_rate: 6.5498e-05\n",
      "Epoch 13/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 140ms/step - accuracy: 0.9176 - f1_score: 0.9176 - loss: 0.2210 - precision_m: 0.9176 - recall_m: 0.9176 - val_accuracy: 0.7121 - val_f1_score: 0.7037 - val_loss: 0.6785 - val_precision_m: 0.7037 - val_recall_m: 0.7037 - learning_rate: 5.9265e-05\n",
      "Epoch 14/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 144ms/step - accuracy: 0.9263 - f1_score: 0.9263 - loss: 0.1832 - precision_m: 0.9263 - recall_m: 0.9263 - val_accuracy: 0.7121 - val_f1_score: 0.7037 - val_loss: 0.6713 - val_precision_m: 0.7037 - val_recall_m: 0.7037 - learning_rate: 5.3626e-05\n",
      "Epoch 15/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 145ms/step - accuracy: 0.9167 - f1_score: 0.9167 - loss: 0.2105 - precision_m: 0.9167 - recall_m: 0.9167 - val_accuracy: 0.7151 - val_f1_score: 0.7067 - val_loss: 0.6232 - val_precision_m: 0.7067 - val_recall_m: 0.7067 - learning_rate: 4.8522e-05\n",
      "Epoch 16/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 144ms/step - accuracy: 0.9369 - f1_score: 0.9369 - loss: 0.1580 - precision_m: 0.9369 - recall_m: 0.9369 - val_accuracy: 0.7421 - val_f1_score: 0.7452 - val_loss: 0.5572 - val_precision_m: 0.7452 - val_recall_m: 0.7452 - learning_rate: 4.3905e-05\n",
      "Epoch 17/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 143ms/step - accuracy: 0.9399 - f1_score: 0.9399 - loss: 0.1551 - precision_m: 0.9399 - recall_m: 0.9399 - val_accuracy: 0.5772 - val_f1_score: 0.5704 - val_loss: 0.8833 - val_precision_m: 0.5704 - val_recall_m: 0.5704 - learning_rate: 3.9727e-05\n",
      "Epoch 18/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 144ms/step - accuracy: 0.9546 - f1_score: 0.9546 - loss: 0.1093 - precision_m: 0.9546 - recall_m: 0.9546 - val_accuracy: 0.7166 - val_f1_score: 0.7081 - val_loss: 0.6162 - val_precision_m: 0.7081 - val_recall_m: 0.7081 - learning_rate: 3.5946e-05\n",
      "Epoch 19/75\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 141ms/step - accuracy: 0.9478 - f1_score: 0.9478 - loss: 0.1265 - precision_m: 0.9478 - recall_m: 0.9478 - val_accuracy: 0.7766 - val_f1_score: 0.7793 - val_loss: 0.5616 - val_precision_m: 0.7793 - val_recall_m: 0.7793 - learning_rate: 3.2526e-05\n",
      "Epoch 20/75\n",
      "\u001b[1m109/296\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 121ms/step - accuracy: 0.9432 - f1_score: 0.9432 - loss: 0.1293 - precision_m: 0.9433 - recall_m: 0.9433"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hist1 = SAR_CNN.fit(S1_dataset_tr,\n",
    "                    validation_data = S1_dataset_val,\n",
    "                    epochs = EPOCHS,\n",
    "                    callbacks = callbacks_1\n",
    "                   )\n",
    "\n",
    "\n",
    "#save model\n",
    "sar_model_path = 'CNN_models/SAR_CNN.h5'\n",
    "SAR_CNN.save(filepath = 'CNN_models/SAR_CNN.h5')\n",
    "\n",
    "\n",
    "#plot history \n",
    "plot_history(hist1,'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAR_CNN.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Evaluate on validation dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-07T15:27:41.987058Z",
     "iopub.status.busy": "2024-10-07T15:27:41.982588Z",
     "iopub.status.idle": "2024-10-07T15:28:43.520641Z",
     "shell.execute_reply": "2024-10-07T15:28:43.519700Z",
     "shell.execute_reply.started": "2024-10-07T15:27:41.986964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.8285 - f1_score: 0.8283 - loss: 0.8320 - precision_m: 0.8283 - recall_m: 0.8283\n"
     ]
    }
   ],
   "source": [
    "SAR_CNN.evaluate(S1_dataset_val)\n",
    "\n",
    "# del SAR_CNN;gc.collect()\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training the RGB - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:06:47.115896Z",
     "iopub.status.busy": "2022-09-26T12:06:47.115283Z",
     "iopub.status.idle": "2022-09-26T12:06:48.907061Z",
     "shell.execute_reply": "2022-09-26T12:06:48.906234Z",
     "shell.execute_reply.started": "2022-09-26T12:06:47.115851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3109502/4156760507.py:27: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(\n"
     ]
    }
   ],
   "source": [
    "RGB_CNN = multichannel_cnn(num_channels = 3,\n",
    "                           hidden_units = 512, #number of  hidden dense\n",
    "                           weights = 'imagenet'\n",
    "                          )\n",
    "\n",
    "\n",
    "RGB_CNN.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss = 'sparse_categorical_crossentropy',\n",
    "                metrics = ['accuracy',f1_score,recall_m,precision_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:06:51.782747Z",
     "iopub.status.busy": "2022-09-26T12:06:51.782268Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea301b/anaconda3/envs/tensor/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_320']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731308893.686633 3131987 service.cc:146] XLA service 0x7fee700028f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731308893.686683 3131987 service.cc:154]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-11-11 15:08:15.230074: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-11 15:08:16.772873: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Incompatible shapes: [9] vs. [9,2]\n",
      "\t [[{{node mul_1}}]]\n",
      "\ttf2xla conversion failed while converting __inference_one_step_on_data_40786[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node mul_1 defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [9] vs. [9,2]\n\t [[{{node mul_1}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_40786[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_42023]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hist2 \u001b[38;5;241m=\u001b[39m \u001b[43mRGB_CNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRGB_dataset_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRGB_dataset_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#save model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m RGB_CNN\u001b[38;5;241m.\u001b[39msave(filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN_models/RGB_CNN.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node mul_1 defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [9] vs. [9,2]\n\t [[{{node mul_1}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_40786[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_42023]"
     ]
    }
   ],
   "source": [
    "hist2 = RGB_CNN.fit(RGB_dataset_tr,\n",
    "                    validation_data = RGB_dataset_val,\n",
    "                    epochs = EPOCHS,\n",
    "                    callbacks = callbacks_1)\n",
    "\n",
    "#save model\n",
    "\n",
    "RGB_CNN.save(filepath = 'CNN_models/RGB_CNN.h5')\n",
    "\n",
    "plot_history(hist2,'f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete \n",
    "del RGB_CNN;gc.collect()\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the GradCAM and saliency Maps for RGB CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-09-26T11:18:35.447909Z",
     "iopub.status.idle": "2022-09-26T11:18:35.44864Z",
     "shell.execute_reply": "2022-09-26T11:18:35.448374Z",
     "shell.execute_reply.started": "2022-09-26T11:18:35.448349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.subplots(4,4,figsize=(8*3,8*3))\n",
    "n = 4\n",
    "idx= 1\n",
    "\n",
    "\n",
    "for images,labels in RGB_dataset_val.shuffle(buffer_size=12).take(1):\n",
    "\n",
    "    for i in range(4):\n",
    "        #get label \n",
    "        img = images[i]\n",
    "        lab = int(labels[i].numpy())\n",
    "\n",
    "\n",
    "#         print(img.shape,lab.shape)\n",
    "        score1 = CategoricalScore(lab)\n",
    "\n",
    "\n",
    "\n",
    "        #predict on image\n",
    "        prd= np.argmax(RGB_CNN.predict(img[tf.newaxis,:,:,:]))\n",
    "\n",
    "\n",
    "        plt.subplot(4,4,idx)\n",
    "        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        idx+=1\n",
    "\n",
    "        #saliency\n",
    "\n",
    "        plt.subplot(4,4,idx)\n",
    "        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n",
    "        sal = get_saliency(img,\n",
    "                           score1,\n",
    "                           cnn_model = RGB_CNN).squeeze(axis=0)\n",
    "        \n",
    "#         print(sal.shape)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(sal,alpha=0.45,cmap='jet') #overlay\n",
    "        idx+=1\n",
    "\n",
    "        #gradcam\n",
    "        plt.subplot(4,4,idx)\n",
    "        gdcam = get_gradcam(img,\n",
    "                            score1,\n",
    "                           cnn_model = RGB_CNN)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n",
    "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n",
    "        plt.axis('off')\n",
    "        idx+=1\n",
    "\n",
    "\n",
    "        #gradcam ++\n",
    "        plt.subplot(4,4,idx)\n",
    "        gdcam_pls = get_gradcam_plus(img,\n",
    "                                     score1,\n",
    "                                     model = RGB_CNN)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n",
    "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n",
    "        plt.axis('off')\n",
    "        idx+=1\n",
    "\n",
    "        if idx>16:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-09-26T11:18:35.449816Z",
     "iopub.status.idle": "2022-09-26T11:18:35.45048Z",
     "shell.execute_reply": "2022-09-26T11:18:35.450276Z",
     "shell.execute_reply.started": "2022-09-26T11:18:35.450252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(4,4,figsize=(8*3,8*3))\n",
    "n = 4\n",
    "idx= 1\n",
    "\n",
    "\n",
    "for images,labels in RGB_dataset_val.shuffle(buffer_size=12).take(1):\n",
    "\n",
    "    for i in range(4):\n",
    "        #get label \n",
    "        img = images[i]\n",
    "        lab = int(labels[i].numpy())\n",
    "\n",
    "\n",
    "#         print(img.shape,lab.shape)\n",
    "        score1 = CategoricalScore(lab)\n",
    "\n",
    "\n",
    "\n",
    "        #predict on image\n",
    "        prd= np.argmax(RGB_CNN.predict(img[tf.newaxis,:,:,:]))\n",
    "\n",
    "\n",
    "        plt.subplot(4,4,idx)\n",
    "        plt.title(f'orignal image ({CFG.class_dict[lab]})')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        idx+=1\n",
    "\n",
    "        #saliency\n",
    "\n",
    "        plt.subplot(4,4,idx)\n",
    "        plt.title(f'predicted {CFG.class_dict[prd]}(saliency map)')\n",
    "        sal = get_saliency(img,\n",
    "                           score1,\n",
    "                           cnn_model = RGB_CNN).squeeze(axis=0)\n",
    "        \n",
    "#         print(sal.shape)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(sal,alpha=0.45,cmap='jet') #overlay\n",
    "        idx+=1\n",
    "\n",
    "        #gradcam\n",
    "        plt.subplot(4,4,idx)\n",
    "        gdcam = get_gradcam(img,\n",
    "                            score1,\n",
    "                           cnn_model = RGB_CNN)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(gdcam,alpha=0.30,cmap='jet') #overlay\n",
    "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam)')\n",
    "        plt.axis('off')\n",
    "        idx+=1\n",
    "\n",
    "\n",
    "        #gradcam ++\n",
    "        plt.subplot(4,4,idx)\n",
    "        gdcam_pls = get_gradcam_plus(img,\n",
    "                                     score1,\n",
    "                                     model = RGB_CNN)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(gdcam_pls,alpha=0.30,cmap='jet') #overlay\n",
    "        plt.title(f'predicted {CFG.class_dict[prd]}(gradcam + +)')\n",
    "        plt.axis('off')\n",
    "        idx+=1\n",
    "\n",
    "        if idx>16:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2227758,
     "sourceId": 3725387,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30197,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
